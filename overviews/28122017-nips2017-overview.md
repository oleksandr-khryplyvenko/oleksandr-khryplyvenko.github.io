### The following overview covers only presentations I visited<br>It reflects my personal oppinion

### [click here to view all NIPS 2017 videos](https://www.facebook.com/nipsfoundation/videos)

<table>
<tbody>
<th>link</th>
<th>authors</th>
<th>my comments</th>
<th>tags</th>
<tr>
    <td><a href="https://www.facebook.com/nipsfoundation/videos/1552060484885185/">Deep Learning: Practice and Trends</a></td>
    <td width="170px">Nando de Freitas <br>Scott Reed <br>Oriol Vinyals</td>
    <td>· hints how to design architectures for various tasks <br> · hints how to build losses <br> · hints how to cook and feed your data <br>· brief overview how some ideas were put together to make well-known architectures</td>
    <td width="160px">graphs<br>probabilistic ML<br>GAN<br>cause & effect<br>metalearning</td>
</tr>
<tr>
    <td><a href="https://www.facebook.com/nipsfoundation/videos/1552223308202236/">Deep Probabilistic Modelling with Gaussian Processes</a></td>
    <td width="170px">Neil D Lawrence</td>
    <td>· what probabilistic ML can suggest<br> · where it's better and where it's worse<br> · restrictions within probabilistic ML</td>
    <td width="160px">probabilistic ML</td>
</tr>
<tr>
    <td><a href="/overviews/graphs_and_manifolds_reducedsize.pdf">Geometric Deep Learning on Graphs and Manifolds</a></td>
    <td width="170px">Michael Bronstein<br>Joan Bruna<br>Arthur Szlam<br>Xavier Bresson<br>Yann LeCun</td>
    <td>· how to represent graphs and manifolds for ML</td>
    <td width="160px">graphs<br>matrix completion<br>manifolds</td>
</tr>
<tr>
    <td><a href="https://www.facebook.com/nipsfoundation/videos/1553236368100930/">start at 56:25 Ali Rahimi's "take AI from alchemy to electricity" speech</a></td>
    <td width="170px">Ali Rahimi<br>Benjamin Recht</td>
    <td>· how to break empiric ML(examples)<br>· why interpretability is important</td>
    <td width="160px">interpretability</td>
</tr>
<tr>
    <td><a href="http://papers.nips.cc/paper/7106-online-learning-with-transductive-regret.pdf">Online Learning with Transductive Regret</a></td>
    <td width="170px">Mehryar Mohri<br>Scott Yang</td>
    <td>· regret types generalisation</td>
    <td width="160px">RL</td>
</tr>
<tr>
    <td>Safe and Nested Subgame Solving for Imperfect-Information Games<br><a href="https://www.youtube.com/watch?v=EbKmZLp5HvA&feature=youtu.be">video</a><br><a href="http://papers.nips.cc/paper/6671-safe-and-nested-subgame-solving-for-imperfect-information-games">paper</a></td>
    <td width="170px">Noam Brown<br>Tuomas Sandholm</td>
    <td>· imperfect information games with examples<br>· how to beat humans in poker</td>
    <td width="160px">RL</td>
</tr>
<tr>
    <td><a href="http://papers.nips.cc/paper/6846-information-theoretic-analysis-of-generalization-capability-of-learning-algorithms.pdf">Information-theoretic analysis of generalization capability of learning algorithms</a></td>
    <td width="170px">Aolin Xu<br>Maxim Raginsky</td>
    <td> title speaks for itself</td>
    <td width="160px">interpretability</td>
</tr>
<tr>
    <td><a href="http://papers.nips.cc/paper/6928-clustering-billions-of-reads-for-dna-data-storage.pdf">Clustering Billions of Reads for DNA Data Storage</a></td>
    <td width="170px">Cyrus Rashtchiana<br>Konstantin Makarychev<br>Miklós Rácz<br>Siena Dumas Ang<br>Djordje Jevdjic<br>Sergey Yekhanin<br>Luis Ceze<br>Karin Strauss</td>
    <td>· novel & fast clustering algorithm</td>
    <td width="160px">clusterisation</td>
</tr>
<tr>
    <td><a href="http://papers.nips.cc/paper/7213-poincare-embeddings-for-learning-hierarchical-representations.pdf">Poincaré Embeddings for Learning Hierarchical Representations</a></td>
    <td width="170px">Maximilian Nickel<br>Douwe Kiela</td>
    <td>· another way to represent graphs in ML</td>
    <td width="160px">graphs<br>interpretability</td>
</tr>
<tr>
    <td>A Linear-Time Kernel Goodness-of-Fit Test<br><a href="http://wittawat.com/assets/poster/kgof_nips2017_poster.pdf">poster</a><br><a href="http://papers.nips.cc/paper/6630-a-linear-time-kernel-goodness-of-fit-test.pdf">article</a><br><a href="/overviews/model_fit.pdf">slides</a></td>
    <td width="170px">Wittawat Jitkrittum<br>Wenkai Xu<br>Zoltán Szabó<br>Kenji Fukumizu<br>Arthur Gretton</td>
    <td>· how good your model fits the data<br>· and <b>where</b> it gets wrong </td>
    <td width="160px">probabilistic ML<br>interpretability</td>
</tr>
<tr>
    <td>Generalization Properties of Learning with Random Features<br><a href="http://papers.nips.cc/paper/6914-generalization-properties-of-learning-with-random-features.pdf">article</a><br><a href="/overviews/random_features.pdf">slides</a></td>
    <td width="170px">Alessandro Rudi<br>Lorenzo Rosasco<br></td>
    <td>· lower computational cost<br>· with no loss in generalisation<br>· optimize with respect to introduced coefficients</td>
    <td width="160px">kernels<br>interpretability</td>
</tr>




</tbody>
</table>



- interpretability
- probabilistic ML
- GANs
- Reinforcement learning
- meta learning
- message passing networks
- representing graphs in modern models
    - convolutional networks on graphs (Duvenaud et. al. NIPS 2015)
- cause and effect
    - neural program induction<br>
      learning to execute (Zaremba & Sutskever, 2014)
    - DNC(differentiable neural computers)