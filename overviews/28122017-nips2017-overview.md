### The following overview covers only presentations I visited<br>It reflects my personal oppinion

### [click here to view all NIPS 2017 videos](https://www.facebook.com/nipsfoundation/videos)

Some videos contain ~ 10 posters/presentations, or even more.
So virtually everything presented @ NIPS is available there.

http://interpretable.ml/


<table>
<tbody>
<th>link</th>
<th>authors</th>
<th>my comments</th>
<th>tags</th>
<tr>
    <td><a href="https://www.facebook.com/nipsfoundation/videos/1552060484885185/">Deep Learning: Practice and Trends</a></td>
    <td width="170px">Nando de Freitas <br>Scott Reed <br>Oriol Vinyals</td>
    <td>• hints how to design architectures for various tasks <br>• hints how to build losses <br>• hints how to cook and feed your data <br>• brief overview how some ideas were put together to make well-known architectures</td>
    <td width="160px">graphs<br>probabilistic ML<br>GAN<br>cause & effect<br>metalearning</td>
</tr>
<tr>
    <td><a href="https://www.facebook.com/nipsfoundation/videos/1552223308202236/">Deep Probabilistic Modelling with Gaussian Processes</a></td>
    <td width="170px">Neil D Lawrence</td>
    <td>• what probabilistic ML can suggest<br>• where it's better and where it's worse<br>• restrictions within probabilistic ML</td>
    <td width="160px">probabilistic ML</td>
</tr>
<tr>
    <td><a href="/overviews/graphs_and_manifolds_reducedsize.pdf">Geometric Deep Learning on Graphs and Manifolds</a></td>
    <td width="170px">Michael Bronstein<br>Joan Bruna<br>Arthur Szlam<br>Xavier Bresson<br>Yann LeCun</td>
    <td>• how to represent graphs and manifolds for ML</td>
    <td width="160px">graphs<br>matrix completion<br>manifolds</td>
</tr>
<tr>
    <td><a href="https://www.facebook.com/nipsfoundation/videos/1553236368100930/">start at 56:25 Ali Rahimi's "take AI from alchemy to electricity" speech</a></td>
    <td width="170px">Ali Rahimi<br>Benjamin Recht</td>
    <td>• how to break empiric ML(examples)<br>• why interpretability is important</td>
    <td width="160px">interpretability</td>
</tr>
<tr>
    <td><a href="http://papers.nips.cc/paper/7106-online-learning-with-transductive-regret.pdf">Online Learning with Transductive Regret</a></td>
    <td width="170px">Mehryar Mohri<br>Scott Yang</td>
    <td>• regret types generalisation</td>
    <td width="160px">RL</td>
</tr>
<tr>
    <td>Safe and Nested Subgame Solving for Imperfect-Information Games<br><a href="https://www.youtube.com/watch?v=EbKmZLp5HvA&feature=youtu.be">video</a><br><a href="http://papers.nips.cc/paper/6671-safe-and-nested-subgame-solving-for-imperfect-information-games">paper</a></td>
    <td width="170px">Noam Brown<br>Tuomas Sandholm</td>
    <td>• imperfect information games with examples<br>• how to beat humans in poker</td>
    <td width="160px">RL</td>
</tr>
<tr>
    <td><a href="http://papers.nips.cc/paper/6846-information-theoretic-analysis-of-generalization-capability-of-learning-algorithms.pdf">Information-theoretic analysis of generalization capability of learning algorithms</a></td>
    <td width="170px">Aolin Xu<br>Maxim Raginsky</td>
    <td> title speaks for itself</td>
    <td width="160px">interpretability</td>
</tr>
<tr>
    <td><a href="http://papers.nips.cc/paper/6928-clustering-billions-of-reads-for-dna-data-storage.pdf">Clustering Billions of Reads for DNA Data Storage</a></td>
    <td width="170px">Cyrus Rashtchiana<br>Konstantin Makarychev<br>Miklós Rácz<br>Siena Dumas Ang<br>Djordje Jevdjic<br>Sergey Yekhanin<br>Luis Ceze<br>Karin Strauss</td>
    <td>• novel & fast clustering algorithm</td>
    <td width="160px">clusterisation</td>
</tr>
<tr>
    <td><a href="http://papers.nips.cc/paper/7213-poincare-embeddings-for-learning-hierarchical-representations.pdf">Poincaré Embeddings for Learning Hierarchical Representations</a></td>
    <td width="170px">Maximilian Nickel<br>Douwe Kiela</td>
    <td>• another way to represent graphs in ML</td>
    <td width="160px">graphs<br>interpretability</td>
</tr>
<tr>
    <td>A Linear-Time Kernel Goodness-of-Fit Test<br><a href="http://wittawat.com/assets/poster/kgof_nips2017_poster.pdf">poster</a><br><a href="http://papers.nips.cc/paper/6630-a-linear-time-kernel-goodness-of-fit-test.pdf">article</a><br><a href="/overviews/model_fit.pdf">slides</a></td>
    <td width="170px">Wittawat Jitkrittum<br>Wenkai Xu<br>Zoltán Szabó<br>Kenji Fukumizu<br>Arthur Gretton</td>
    <td>• how good your model fits the data<br>• and <b>where</b> it gets wrong </td>
    <td width="160px">probabilistic ML<br>interpretability</td>
</tr>
<tr>
    <td>Generalization Properties of Learning with Random Features<br><a href="http://papers.nips.cc/paper/6914-generalization-properties-of-learning-with-random-features.pdf">article</a><br><a href="/overviews/random_features.pdf">slides</a></td>
    <td width="170px">Alessandro Rudi<br>Lorenzo Rosasco</td>
    <td>• lower computational cost<br>• with no loss in generalisation<br>• optimize with respect to introduced coefficients</td>
    <td width="160px">kernels<br>interpretability</td>
</tr>
<tr>
    <td>Train longer, generalize better: closing the generalization gap in large batch training of neural networks<br><a href="http://papers.nips.cc/paper/6770-train-longer-generalize-better-closing-the-generalization-gap-in-large-batch-training-of-neural-networks.pdf">article</a><br><a href="https://github.com/eladhoffer/bigBatch">code</a><br><a href="https://www.youtube.com/watch?v=BrAu1gZVCP4&feature=youtu.be">video</a></td>
    <td width="170px">Elad Hoffer<br>Itay Hubara<br> Daniel Soudry</td>
    <td>• 'generalisation gap' phenomenon analysis and solution for it</td>
    <td width="160px">batches<br>training<br>interpretability</td>
</tr>
<tr>
    <td><a href="http://papers.nips.cc/paper/6969-end-to-end-differentiable-proving.pdf">End-to-End Differentiable Proving</a></td>
    <td width="170px">Tim Rocktäschel<br>Sebastian Riedel</td>
    <td>• prolog & gradient methods synthesis</td>
    <td width="160px">cause & effect<br>prolog</td>
</tr>
<tr>
    <td><a href="https://www.facebook.com/nipsfoundation/videos/vb.375737692517476/1554402331317667/?type=2&theater">Gradient descent GAN optimization is locally stable (start @ 42:00)</a></td>
    <td width="170px">Vaishnavh Nagarajan<br>J. Zico Kolter</td>
    <td>• Good analysis why GANs are instable and how to work with them to overcome this issue</td>
    <td width="160px">GAN<br>interpretability</td>
</tr>
<tr>
    <td><a href="https://www.facebook.com/nipsfoundation/videos/1554741347950432/">Inverse Reward Design (start @ 31:00)</a></td>
    <td width="170px">Dylan Hadfield-Menell<br>Smitha Milli<br>Pieter Abbeel<br>Stuart J Russell<br>Anca Dragan</td>
    <td>• very interesting overview how to take into account states and rewards unseen during training<br>• how to design rewards in more proper way</td>
    <td width="160px">RL</td>
</tr>
<tr>
    <td><a href="https://www.facebook.com/nipsfoundation/videos/1554741347950432/">Inverse Reward Design (start @ 31:00)</a></td>
    <td width="170px">Dylan Hadfield-Menell<br>Smitha Milli<br>Pieter Abbeel<br>Stuart J Russell<br>Anca Dragan</td>
    <td>• very interesting overview how to take into account states and rewards unseen during training<br>• how to design rewards in more proper way</td>
    <td width="160px">RL</td>
</tr>
<tr>
    <td><a href="https://www.facebook.com/nipsfoundation/videos/1554741347950432/">Unifying PAC and Regret: Uniform PAC Bounds for Episodic Reinforcement Learning (start @ 54:50)</a></td>
    <td width="170px">Christoph Dann<br>Tor Lattimore<br>Emma Brunskill</td>
    <td></td>
    <td width="160px">RL</td>
</tr>
<tr>
    <td><a href="https://www.facebook.com/nipsfoundation/videos/1554741347950432/">Natural Value Approximators: Learning when to Trust Past Estimates (start @ 1:10:00)</a></td>
    <td width="170px">Zhongwen Xu<br>Joseph Modayil<br>Hado van Hasselt<br>Andre Barreto<br>David Silver<br>Tom Schaul</td>
    <td>• reward function is not smooth in general<br>• proposed a way to approximate it better</td>
    <td width="160px">RL</td>
</tr>
<tr>
    <td><a href="https://www.facebook.com/nipsfoundation/videos/1555427447881822/">Learning State Representations</a></td>
    <td width="170px">Yael Niv</td>
    <td>• observations are clustered to form states<br>• hints how it's done from math & biological point of view</td>
    <td width="160px">RL<br>probabilistic ML<br>state representation</td>
</tr>
<tr>
    <td><a href="https://www.facebook.com/nipsfoundation/videos/1555551597869407/">Quantifying how much sensory information in a neural code is relevant for behavior (30:50)</a></td>
    <td width="170px">Giuseppe Pica<br>Eugenio Piasini<br>Houman Safaai<br>Caroline Runyan<br>Christopher Harvey<br>Mathew Diamond<br>Christoph Kayser<br>Tommaso Fellin<br>Stefano Panzeri</td>
    <td>• observations are clustered to form states<br>• hints how it's done from math point of view</td>
    <td width="160px">RL<br>probabilistic ML<br>state representation</td>
</tr>
<tr>
    <td><a href="https://www.facebook.com/nipsfoundation/videos/1555551597869407/">Toward Goal-Driven Neural Network Models for the Rodent Whisker-Trigeminal System</a></td>
    <td width="170px">Chengxu Zhuang<br>Jonas Kubilius<br>Mitra JZ Hartmann<br>Daniel Yamins</td>
    <td>• imagine you design an agent retrieving information from a new-type sensory domain. Then this lecture will be interesting for you</td>
    <td width="160px">sensors<br>encoding shape</td>
</tr>
<tr>
    <td colspan="4" align="center">
        <b>Interpretable ML</b><br>
        Motivations for Interpretability<br>
    </td>
</tr>
    <td colspan="4" align="left">
        • Sense-check: How can an analyst make sure that the prediction is sound?<br>
        • Fairness: How to ensure absence of bias?<br>
        • Compliance: How to provide required explanations?<br>
        • Iteration: How can the model’s predictions be improved?<br>
        • Bottom line: ML will not move into regulated sectors without (some level of interpretability
    </td>
</tr>
<tr>
    <td>The role of causality for interpretability<br><a href="http://s.interpretable.ml/nips_interpretable_ml_2017_Bernhard_Schoelkopf.pdf">slides</a><br><a href="https://www.youtube.com/watch?v=9C3RvDs_hHw&feature=youtu.be">video</a></td>
    <td width="170px">Bernhard Scholkopf</td>
    <td>• why causality is important<br>• how to recognize causality<br>• which models are interpretable</td>
    <td width="160px">causality<br>interpretability</td>
</tr>
<tr>
    <td>Interpretable Discovery in Large Image Data Sets<br><a href="http://s.interpretable.ml/nips_interpretable_ml_2017_kiri_wagstaff.pdf">slides</a><br><a href="https://www.youtube.com/watch?v=_K2wVfi_KDM">video</a></td>
    <td width="170px">Kiri Wagstaff</td>
    <td>• novelty discovery</td>
    <td width="160px">interpretability<br>clusterization<br>state representation</td>
</tr>
<tr>
    <td>The (hidden) Cost of Calibration<br><a href="http://s.interpretable.ml/nips_interpretable_ml_2017_Kilian_Weinberger.pdf">slides</a><br><a href="https://www.youtube.com/watch?v=fDtQQ9GlSJY">video</a></td>
    <td width="170px">Kilian Weinberger</td>
    <td>• explains the nature of bias and how to (partially) deal with it<br>• bias is inherently unavoidable in ML</td>
    <td width="160px">interpretability<br>bias</td>
</tr>


</tbody>
</table>



- interpretability
- probabilistic ML
- GANs
- Reinforcement learning
- meta learning
- message passing networks
- representing graphs in modern models
    - convolutional networks on graphs (Duvenaud et. al. NIPS 2015)
- cause and effect
    - neural program induction<br>
      learning to execute (Zaremba & Sutskever, 2014)
    - DNC(differentiable neural computers)