### The following overview covers only presentations I visited<br>It reflects my personal oppinion

<table>
<tbody>
<th>link</th>
<th>authors</th>
<th>my comments</th>
<th>tags</th>
<tr>
    <td><a href="https://www.facebook.com/nipsfoundation/videos/1552060484885185/">Deep Learning: Practice and Trends</a></td><td width="120px">Nando de Freitas <br>Scott Reed <br>Oriol Vinyals</td><td>· hints how to design architectures for various tasks <br> · hints how to build losses <br> · hints how to cook and feed your data <br>· brief overview how some ideas were put together to make well-known architectures</td><td width="120px">graphs<br>probabilistic ML<br>GAN<br>cause & effect<br>metalearning</td>
</tr>
<tr>
    <td><a href="https://www.facebook.com/nipsfoundation/videos/1552223308202236/">Deep Probabilistic Modelling with Gaussian Processes</a></td><td width="120px">Neil D Lawrence</td><td>· what probabilistic ML can suggest<br> · where it's better and where it's worse<br> · restrictions within probabilistic ML</td><td width="120px">probabilistic ML</td>
</tr>
<tr>
    <td><a href="/overviews/graphs_and_manifolds_reducedsize.pdf">Geometric Deep Learning on Graphs and Manifolds</a></td><td width="120px">Michael Bronstein<br>Joan Bruna<br>Arthur Szlam<br>Xavier Bresson<br>Yann LeCun</td><td>· how to represent graphs and manifolds for ML</td><td width="120px">graphs<br>matrix completion<br>manifolds</td>
</tr>
</tbody>
</table>


- interpretability
- probabilistic ML
- GANs
- Reinforcement learning
- meta learning
- message passing networks
- representing graphs in modern models
    - convolutional networks on graphs (Duvenaud et. al. NIPS 2015)
- cause and effect
    - neural program induction<br>
      learning to execute (Zaremba & Sutskever, 2014)
    - DNC(differentiable neural computers)